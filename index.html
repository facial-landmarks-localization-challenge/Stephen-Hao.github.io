<!DOCTYPE html>
<html lang="en" style="overflow: scroll; height: 100%;">

<head>
    <meta charset="utf-8">
    <title>Grand Challenge of 106-p Facial Landmark Localization</title>
    <link rel="stylesheet" href="src/css/jquery.fullPage.css">
    <link rel="stylesheet" href="src/css/ai-competition.css">
</head>

<body>
    <div class="navigation">
        <ul id="menu">
            <li class="logo"><img src="src/assets/images/logo.png" /></li>
            <li data-menuanchor="index" class="active"><a href="#index">Home</a></li>
            <li data-menuanchor="introduction" class=""><a href="#introduction">Introduction</a></li>
            <li data-menuanchor="fashion-style" class=""><a href="#fashion-style">Challenge</a></li>
            <li data-menuanchor="prize" class=""><a href="#prize">Prize</a></li>
            <li data-menuanchor="comp-date" class=""><a href="#comp-date">Deadline </a></li>
            <li data-menuanchor="organizer" class=""><a href="#organizer">Coordinators</a></li>
            <li data-menuanchor="contact-us" class=""><a href="#contact-us">Contacts</a></li>
        </ul>
    </div>
    <div id="content_container">
        <div class="section fp-section fp-table active" data-anchor="index">
            <div class="fp-tableCell">
                

                        <h3 style="text-align:center">Grand Challenge of 106-p Facial Landmark Localization</h3>
                        <h5 style="text-align:center">ICME2019</h5>
              
                
                <a class="sign-btn" href="https://wj.qq.com/s/2777865/fa45">Click to sign up</a>
            </div>
            
        </div>
                <div class="section fp-section fp-table" data-anchor="introduction">
            <div class="container introduction-content">
                <ul class="introduction-list">
                        <b>Introduction</b>
                    <li>
                        <p>As the deep learning methods have been largely developed in facial landmark localization task, the requirements of practical applications are growing fast. However, for large poses and occlusion, the accuracy of localization  needs to be improved. Here, JD AI Research and NLPR, CASIA sincerely invited researchers and developers from academia and industry to participate in this competition and encourage further discussion on technical and application issues.
                        </p>
                    </li>
                    <li>
                        <p><b>Official email: facial_lmgc_icme@163.com  </b></p>
                        <p><b>News</b></p>
                    </li>
                    <li>
                        <p>2019/2/20 - Please note that all the participants should submit the binary or model along with the technical report to the organizer (facial_lmgc_icme@163.com) before April 8th, 2019. The purpose of the tech report is to validate the performance (without outside training data) of proposed algorithms from the participants. The participants should introduce the algorithms and the experimental configurations clearly in the technical report. The format of technical report is unlimited, but we highly recommend you to adopt the template offered by ICME from the website (http://www.icme2018.org/author_info). All the submitted binaries will be evaluated on the Test Dataset 2 and the top three teams will be awarded. We will write the final report for this challenge, in which the algorithms of the top three teams will be descripted and the related participants will be taken as the collaboration authors. This paper will be published on ICME workshop.</p>
                        <p>2018/12/29 - At the request of some participants, we have appropriately cropped each test image on the basis of the detection bounding box, generated by our face detector, which is same as used in the training set (Note: Our detector is trained on the WIDER FACE, at the same time, we expanded the width and height outward by 1/8 on the generated detection box.). Our test interface still remains unchanged, your model needs to accept two parameters as input and be executed like 
                        <p> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp "./Binary_filename  parameter1	parameter2"</p> Here, parameter1 refers to the absolute path for the cropped image (.jpg) and parameter2 refers to the absolute path for the output file (.txt). Of course you can use any other face detection method you prefer, then the parameter1 refers to the absolute path for the original image (.jpg), and your submitted items should include your detector. Please see the “Details” document for details.</p>
                        <p>2018/12/18 - We have released the baseline evaluated on the test dateset 1, you can check them by clicking “Leaderboard” on the homepage. Please refer to the “Details” on the homepage for the evaluation criteria.</p>
                        <p>2018/12/4 - You can start submitting the model or binary and corresponding runtime environment with a brief method description to: facial_lmgc_icme@163.com and the organizers will evaluate them on the validation set and send the performance to you. The input and output format are given in the document. Please visit the follow website for details: <a href="https://github.com/facial-landmarks-localization-challenge/facial-landmarks-localization-challenge.github.io/blob/master/Information_of_ICME2019_GC_facial_landmarks_localization__.docx?raw=true" target="_blank"> Details</a></p>
                    	<p>2018/12/4 - We have provided the face detection bounding boxes in the training set by our face detector as a reference. Click on this link to download the bounding box files : <a href="https://github.com/facial-landmarks-localization-challenge/facial-landmarks-localization-challenge.github.io/blob/master/training_dataset_face_detection_bounding_box_v1.zip?raw=true" target="_blank"> Download</a></p>     
                    	<p>2018/11/26 - We are very sorry that there are some wrong landmark. TXT files in the data set. You can click on this link to download the corrected landmark files:<a href="https://github.com/facial-landmarks-localization-challenge/facial-landmarks-localization-challenge.github.io/blob/master/Corrected_landmark.zip?raw=true" target=_blank"> Download</a></p>
                    	<p>2018/11/26 - The key rule: You can only use the data we provide for the competition. DO NOT use external data.</p>
                        <p>2018/11/16 - The competition has officially started and the training data has been released! Now, you can register for the competition to get training data.</p>
                
                    </li>
                </ul>
            </div>

        </div>
        
        
        <div class="section fp-section fp-table" data-anchor="fashion-style">
            <div class="container">
                <ul class="fashion-style-text">
                    <li>
                        <p>Facial landmark localization serves as a key step for many face applications, such as face recognition, emotion estimation and face reconstruction. The objective of facial landmark localization is to predict the coordinates of a set of pre-defined key points on human face. 106-key-point landmarks enable abundant geometric information for face analysis tasks. The purpose of this competition is to promote the development of research on face landmark localization, especially dealing with the complex situations, e.g. large face poses, extreme expressions and occlusions.
                        </p>
                    </li>
                </ul>
                <div class="btn-container">
                    <div class="detail-intro"><a href="https://github.com/facial-landmarks-localization-challenge/facial-landmarks-localization-challenge.github.io/blob/master/Information_of_ICME2019_GC_facial_landmarks_localization__.docx?raw=true">Details</a></div>
                    <div class="related-link"><a href="https://ibug.doc.ic.ac.uk/resources/300-W/">Related link</a></div>
                    <div class="ranking-detail"><a href="leaderboard.html">Leaderboard</font></a></div>
                    <div class="sample-data"><a href=" ">Report</a></div>
                </div>
                <div class=""fashion-style-img">
                    <img src="src/assets/images/data1234567890.png">
                </div>
            </div>
        </div>
        <div class="section fp-section fp-table prize-section" data-anchor="prize">
            <div class="container prize-list">
                <p class="comp-date">According to the final result, the first prize, second prize and third prize will be established.</p>
                <div class="prize-bg prize-text">
                    <img src="src/assets/images/prize1.png">
                    <p>First prize</p>
                    <p>Cash: 1000 USD & JD cloud voucher: 10,000 RMB</p>
                </div>
                <div class="prize-bg prize-text">
                    <img src="src/assets/images/prize2.png">
                    <p>Second prize</p>
                    <p>Cash: 500 USD & JD cloud voucher: 5000 RMB</p>
                </div>
                <div class="prize-bg prize-text">
                    <img src="src/assets/images/prize3.png">
                    <p>Third prize</p>
                    <p>Cash: 500 USD</p>
                </div>
            </div>
        </div>
        <div class="section fp-section fp-table" data-anchor="comp-date">
            <div class="container">
                <p class="comp-date">From now on, we start the competition registration and release training data.</p>
                <table>
                    <tr bgcolor="#F5F7F9">
                        <td>2018/11/16</td>
                        <td>Challenge registration start</td>
                    </tr>
                    <tr>
                        <td>2018/12/1 - 2019/4/1</td>
                        <td>Test 1(Phase validation)</td>
                    </tr>
                    <tr bgcolor="#F5F7F9">
                        <td>2019/4/8</td>
                        <td>Test 2(Phase final evaluation): Model & paper submission deadline</td>
                    </tr>
                    <tr>
                        <td>2019/4/22</td>
                        <td>Paper acceptance notification</td>
                    </tr>
                    <tr bgcolor="#F5F7F9">
                        <td>2019/4/22</td>
                        <td>Final evaluation results announcement</td>
                    </tr>
                    <tr>
                        <td>2019/4/29</td>
                        <td>Camera-ready paper submission deadline</td>
                    </tr>
                </table>
                <div class = "container tips">
                    Tips: You must register for the competition to get training data, and you are not allowed to use external data. The submission model must be Tensorflow, Pytorch, Caffe, Caffe2, MXNet and a detailed description(including preprocessing, etc.).
                    
                </div>
            </div>
        </div>
        <div class="section fp-section fp-table prize-section" data-anchor="organizer">
            <div class="container organizer-list">
                <div class="organizer-bg prize-text">
                    <img src="src/assets/images/shihailin.png">
                    <p>Dr. Hailin Shi</p>
                    <p>JD AI Platform and Research</p>
                </div>
                <div class="organizer-bg prize-text">
                    <img src="src/assets/images/wangxiaobo.jpg">
                    <p>Dr. Xiaobo Wang</p>
                    <p>JD AI Platform and Research</p>
                </div>
                <div class="organizer-bg prize-text">
                    <img src="src/assets/images/liuyinglu.jpg">
                    <p>Dr. Yinglu Liu</p>
                    <p>JD AI Platform and Research</p>
                </div>
                <div class="organizer-bg prize-text">
                    <img src="src/assets/images/zhuxiangyu.png">
                    <p>Dr. Xiangyu Zhu</p>
                    <p>Chinese Academy of Sciences</p>
                </div>
            </div>
        </div>
        <div class="section fp-section fp-table" data-anchor="contact-us">
            <div class="container contact-us">
                <p><b>Please send model or binary with a brief method description to <span>facial_lmgc_icme@163.com.</span></b></p>
                <p></p>
                <p>Ackownledge to the authors of these datasets:300W [1][2], LFPW [3], AFW [4], HELEN [5] and IBUG [6].
                <p>[1] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic.300 Faces in-the-Wild Challenge: The first facial landmark localization Challenge. InInternational Conference onComputer Vision - Workshops (ICCVW), pages 397–403, 2013.</p>
                <p>[2] C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic. 300 faces in-the-wild challenge: Database and results. IVC, 47:3–18, 2016. 3.</p>
                <p>[3] Belhumeur, P., Jacobs, D., Kriegman, D., Kumar, N.. ‘Localizing parts of faces using a consensus of exemplars’.  In Computer Vision and Pattern Recognition, CVPR. (2011).</p>
                <p>[4] X. Zhu, D. Ramanan.‘Face detection, pose estimation and landmark localization in the wild’, Computer Visionand Pattern Recognition (CVPR) Providence, Rhode Island, June 2012.</p>
                <p>[5] Vuong Le, Jonathan Brandt, Zhe Lin, Lubomir Boudev, Thomas S. Huang. ‘Interactive Facial Feature Localization’, ECCV2012.</p> 
                <p>[6] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic. A semi-automatic methodology for facial landmark annotation. In CVPR, 2013.</p> 
                <img src="src/assets/images/logo8.PNG">
            </div>
            <div class="footer">
                <img src="src/assets/images/logo.png" />
                <div>Copyright© JD.com</div>
            </div>
        </div>
    </div>

    <script src="src/js/libs/jquery-1.9.1.min.js"></script>
    <script src="src/js/libs/jquery.fullpage.min.js"></script>
    <script src="src/js/ai-competition.js"></script>
</body>
                                     
</html>
